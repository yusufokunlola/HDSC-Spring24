{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage A - Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer and Named-entity recognition (NER) tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install the transformer library\n",
    "\n",
    "# pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "nlp = pipeline('ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result: [{'entity': 'I-PER', 'score': 0.9987638, 'index': 3, 'word': 'Yusuf', 'start': 5, 'end': 10}, {'entity': 'I-MISC', 'score': 0.799625, 'index': 8, 'word': 'AI', 'start': 25, 'end': 27}, {'entity': 'I-ORG', 'score': 0.8789517, 'index': 12, 'word': 'Ham', 'start': 38, 'end': 41}, {'entity': 'I-ORG', 'score': 0.83856535, 'index': 13, 'word': '##oy', 'start': 41, 'end': 43}, {'entity': 'I-ORG', 'score': 0.98384017, 'index': 14, 'word': '##e', 'start': 43, 'end': 44}, {'entity': 'I-MISC', 'score': 0.8757365, 'index': 22, 'word': 'Art', 'start': 79, 'end': 82}, {'entity': 'I-MISC', 'score': 0.8763754, 'index': 23, 'word': '##ific', 'start': 82, 'end': 86}, {'entity': 'I-MISC', 'score': 0.9533889, 'index': 24, 'word': '##ial', 'start': 86, 'end': 89}, {'entity': 'I-MISC', 'score': 0.9801303, 'index': 25, 'word': 'Intelligence', 'start': 90, 'end': 102}]\n"
     ]
    }
   ],
   "source": [
    "result = nlp(\"I am Yusuf, a Generative AI Intern at Hamoye. I look forward to learning about Artificial Intelligence.\")\n",
    "print(\"result:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity</th>\n",
       "      <th>score</th>\n",
       "      <th>index</th>\n",
       "      <th>word</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I-PER</td>\n",
       "      <td>0.998764</td>\n",
       "      <td>3</td>\n",
       "      <td>Yusuf</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I-MISC</td>\n",
       "      <td>0.799625</td>\n",
       "      <td>8</td>\n",
       "      <td>AI</td>\n",
       "      <td>25</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I-ORG</td>\n",
       "      <td>0.878952</td>\n",
       "      <td>12</td>\n",
       "      <td>Ham</td>\n",
       "      <td>38</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I-ORG</td>\n",
       "      <td>0.838565</td>\n",
       "      <td>13</td>\n",
       "      <td>##oy</td>\n",
       "      <td>41</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I-ORG</td>\n",
       "      <td>0.983840</td>\n",
       "      <td>14</td>\n",
       "      <td>##e</td>\n",
       "      <td>43</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I-MISC</td>\n",
       "      <td>0.875736</td>\n",
       "      <td>22</td>\n",
       "      <td>Art</td>\n",
       "      <td>79</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I-MISC</td>\n",
       "      <td>0.876375</td>\n",
       "      <td>23</td>\n",
       "      <td>##ific</td>\n",
       "      <td>82</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I-MISC</td>\n",
       "      <td>0.953389</td>\n",
       "      <td>24</td>\n",
       "      <td>##ial</td>\n",
       "      <td>86</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I-MISC</td>\n",
       "      <td>0.980130</td>\n",
       "      <td>25</td>\n",
       "      <td>Intelligence</td>\n",
       "      <td>90</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   entity     score  index          word  start  end\n",
       "0   I-PER  0.998764      3         Yusuf      5   10\n",
       "1  I-MISC  0.799625      8            AI     25   27\n",
       "2   I-ORG  0.878952     12           Ham     38   41\n",
       "3   I-ORG  0.838565     13          ##oy     41   43\n",
       "4   I-ORG  0.983840     14           ##e     43   44\n",
       "5  I-MISC  0.875736     22           Art     79   82\n",
       "6  I-MISC  0.876375     23        ##ific     82   86\n",
       "7  I-MISC  0.953389     24         ##ial     86   89\n",
       "8  I-MISC  0.980130     25  Intelligence     90  102"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(result)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer and Question-Answering Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "qna = pipeline('question-answering')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = r\"\"\"\n",
    "            Igbo-Ora is a city and the headquarters of Ibarapa Central, Oyo State, south-western Nigeria, situated 80 kilometres (50 mi) north of Lagos. \n",
    "            In 2006 the population of the town was approximately 72,207 people. In 2017 the population is estimated to be around 278,514 people.\n",
    "            The city is the location of Oyo State College of Agriculture and Technology. \n",
    "            The polytechnic has contributed significantly to the socio-economic and demographic development of the town. \n",
    "            Source: Wikipedia\n",
    "            \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = qna(question='Where is Igboora located?', context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.14106570184230804, 'start': 84, 'end': 152, 'answer': 'south-western Nigeria, situated 80 kilometres (50 mi) north of Lagos'}\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question/context pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.5686454176902771,\n",
       " 'start': 57,\n",
       " 'end': 74,\n",
       " 'answer': 'Hamoye in Bermuda'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "question_answerer = pipeline(\"question-answering\")\n",
    "question_answerer( \n",
    "                question=\"Where do I work?\",\n",
    "                context=\"My name is Yusuf and I work as a Generative AI Intern at Hamoye in Bermuda.\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer and Text Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b81cc27b5624b6286a1e8b2490d73fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.80k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d6e4f74d21b48f394d514371ec88556",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21a1e1015a5648609bc080b239067671",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:06<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7beab5c9d46d481bb5d2ac16694ef23b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "042c211de98b455abc43ce044008e1da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': ' America suffers an increasingly serious decline in the number of engineering graduates and a lack of well-educated engineers . Rapidly developing economies such as China and India, as well as other Industrial countries in Europe and Asia, continue to encourage and advance the teaching of engineering . Both China, respectively, graduate six and eight times as many traditional engineers as does the U.S.'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from transformers import pipeline\n",
    "summarizer = pipeline(\"summarization\")\n",
    "summarizer(\"\"\"\n",
    "           America has changed dramatically during recent years. Not only has the number of graduates in traditional engineering disciplines \n",
    "           such as mechanical, civil, electrical, chemical, and aeronautical engineering declined, but in most of the premier American universities \n",
    "           engineering curricula now concentrate on and encourage largely the study of engineering science. \n",
    "           As a result, there are declining offerings in engineering subjects dealing with infrastructure, the environment, and related issues, and \n",
    "           greater concentration on high technology subjects, largely supporting increasingly complex scientific developments. \n",
    "           While the latter is important, it should not be at the expense of more traditional engineering.\n",
    "           Rapidly developing economies such as China and India, as well as other Industrial countries in Europe and Asia, continue to encourage and \n",
    "           advance the teaching of engineering. Both China and India, respectively, graduate six and eight times as many traditional engineers as does \n",
    "           the United States. Other industrial countries at minimum maintain their output, while America suffers an increasingly serious decline in the \n",
    "           number of engineering graduates and a lack of well-educated engineers.\n",
    "           \"\"\"\n",
    "           )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
